{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste your implementations right here to check your result\n",
    "# (Of course you can add your classes not written here)\n",
    "# return the probability of each classes\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ParseClasses(sequence):\n",
    "    \"\"\"Find there are how many classes in the dataset and calculate the probability of each class\n",
    "\n",
    "    Args:\n",
    "        sequence (1-d array): contains the class label of every data\n",
    "\n",
    "    Returns:\n",
    "        dictionary: key: class name, value: probability\n",
    "    \"\"\"\n",
    "    class_seq = np.unique(sequence)\n",
    "    kclasses = class_seq.shape[0]\n",
    "    prob = {x: (np.count_nonzero(sequence==x) / sequence.shape[0]) for x in class_seq}\n",
    "    print(f\"There are {kclasses} classes in this dataset\")\n",
    "    print(\"The class labels and the probability of each class is\")\n",
    "    print(prob)\n",
    "    return prob\n",
    "    \n",
    "def gini(sequence):\n",
    "    # Class_stat = ParseClasses(sequence=np.array(sequence))\n",
    "    class_seq, cnt = np.unique(sequence, return_counts=True)\n",
    "    p = cnt / sequence.shape[0]\n",
    "    return 1 - np.sum(p ** 2)\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    # Class_stat = ParseClasses(sequence=np.array(sequence))\n",
    "    # prob = np.fromiter(Class_stat.values(), dtype=float)\n",
    "    class_seq, cnt = np.unique(sequence, return_counts=True)\n",
    "    p = cnt / sequence.shape[0]\n",
    "    return (-1) * np.sum(np.where(p == 0, 0, p * np.log2(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "train_df.head()\n",
    "x_train = np.array(train_df)\n",
    "x_test = np.array(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParseClasses(x_train[:,x_train.shape[1]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question():\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        pass\n",
    "\n",
    "    def match(self, row):\n",
    "        val = row[self.column]\n",
    "        return val >= self.value\n",
    "\n",
    "\n",
    "def partition(x_data, question):\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in x_data:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = None\n",
    "        self.root = None\n",
    "        self.feature_count = {}\n",
    "        if criterion == 'gini':\n",
    "            self.measureFunc = gini\n",
    "        else:\n",
    "            self.measureFunc = entropy\n",
    "        return None\n",
    "    class TreeNode():\n",
    "        def __init__(self, rows, gain, question):\n",
    "            self.rows = rows\n",
    "            self.gain = gain\n",
    "            self.question = question\n",
    "            self.pred = None\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            return None\n",
    "        def __init__(self):\n",
    "            self.rows = None\n",
    "            self.gain = None\n",
    "            self.question = None\n",
    "            self.pred = None\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            return None\n",
    "        def print_node_info(self):\n",
    "            print(f'The node\\'s depth is {self.depth}, impurity is {self.impurity}')\n",
    "            print(f'Question is {self.question.column}, and threshold is {self.question.value}')\n",
    "            print(f'{self.left}, {self.right}')\n",
    "            return None\n",
    "    def Informationgain(self, left_rows, right_rows, currentImpurity):\n",
    "        p = float(len(left_rows)) / (len(left_rows) + len(right_rows))\n",
    "        return currentImpurity - p * self.measureFunc(left_rows[:, -1].astype(np.int32)) - (1 - p) * self.measureFunc(right_rows[:, -1].astype(np.int32))\n",
    "\n",
    "    def find_best_split(self, rows):\n",
    "        \"\"\"Find the best split by repeating asking whether a property of a data is greater than thresholds\n",
    "        generated by sorting N data using each property\n",
    "\n",
    "        Args:\n",
    "            rows (N,21): includes 20 properties and 1 columns representing the class of each row.\n",
    "        \"\"\"\n",
    "        best_gain = 0\n",
    "        best_question = None\n",
    "        left_rows = None\n",
    "        right_rows = None\n",
    "        current_impurity = self.measureFunc(rows[:, -1].astype(np.int32))\n",
    "\n",
    "        # for each feature\n",
    "        for col in range(len(rows[0])-1):\n",
    "            # sort data using values in column `col`\n",
    "            # extract the data sorted using current feature\n",
    "            col_sort = rows[np.argsort(rows[:, col])]\n",
    "\n",
    "            # Try N-1 threshold values\n",
    "            for idx in range(len(col_sort)-1):\n",
    "                # i-th and i+1-th sorted value as current threshold\n",
    "                current_threshold = (col_sort[idx, col] + col_sort[idx+1, col]) / 2.0\n",
    "\n",
    "                # is data[col] >= current_threshold ?\n",
    "                # if it's binary, the threshold is 0.5, so it's ok to use '>=' to compare\n",
    "                # if it's real value, also valid to use '>=' to compare\n",
    "                question = Question(column=col,\n",
    "                                    value=current_threshold)\n",
    "\n",
    "                # split the data using current question\n",
    "                # true and false are candidates for best split(potential child nodes)\n",
    "                true_rows  = col_sort[col_sort[:, col] >= current_threshold]\n",
    "                false_rows = col_sort[col_sort[:, col] <  current_threshold]\n",
    "\n",
    "                # # Pick the split that maximize information gain\n",
    "                current_gain = self.Informationgain(left_rows=true_rows,\n",
    "                                                        right_rows=false_rows,\n",
    "                                                        currentImpurity=current_impurity)\n",
    "                # print(current_gain)\n",
    "                if current_gain >= best_gain:\n",
    "                    best_gain, best_question = current_gain, question\n",
    "        label = train_df.columns[best_question.column]\n",
    "        if self.feature_count.get(label) is not None:\n",
    "            self.feature_count[label] = self.feature_count[label] + 1\n",
    "        else:\n",
    "            self.feature_count[label] = 1\n",
    "        return best_gain, best_question\n",
    "    def get_feature_count(self):\n",
    "        print(self.feature_count)\n",
    "        return \n",
    "    def generateTree(self, rows, cur_depth=None):\n",
    "        cur_node = self.TreeNode()\n",
    "        if self.measureFunc(sequence=rows[:, -1]) == 0:\n",
    "            cur_node.pred = int(rows[0, -1])\n",
    "        elif cur_depth == 0:\n",
    "            if np.count_nonzero(rows[:, -1]) >= rows.shape[0] / 2:\n",
    "                cur_node.pred = 1\n",
    "            else:\n",
    "                cur_node.pred = 0\n",
    "        else:\n",
    "            best_gain, best_question = self.find_best_split(rows=rows)\n",
    "            cur_node.rows = rows\n",
    "            cur_node.gain = self.measureFunc(rows[:, -1].astype(np.int32))\n",
    "            cur_node.question = best_question\n",
    "            left_child = rows[rows[:, best_question.column]>=best_question.value]\n",
    "            right_child = rows[rows[:, best_question.column]<best_question.value]\n",
    "            if cur_depth is None:\n",
    "                cur_node.left = self.generateTree(rows=left_child)\n",
    "                cur_node.right = self.generateTree(rows=right_child)\n",
    "            else:\n",
    "                cur_node.left = self.generateTree(rows=left_child, cur_depth=cur_depth-1)\n",
    "                cur_node.right = self.generateTree(rows=right_child, cur_depth=cur_depth-1)\n",
    "        return cur_node\n",
    "    # Generate Tree by fitting data\n",
    "    def fit(self, x_data, y_data):\n",
    "        self.feature_count = {}\n",
    "        y_data = y_data[:, np.newaxis]\n",
    "        rows = np.hstack((x_data, y_data))\n",
    "        self.n_features = len(x_data[0]) - 1\n",
    "        self.root = self.generateTree(rows=rows, cur_depth=self.max_depth)\n",
    "    # After fitting, use the gererated tree to predict x_data\n",
    "    def feature_importance(self):\n",
    "        fi = []\n",
    "        for key in self.feature_count.keys():\n",
    "            fi.append(self.feature_count[key])\n",
    "        return fi\n",
    "    def traverse(self, cur_node, x_data):\n",
    "        if cur_node is None:\n",
    "            return \n",
    "        if cur_node.question is None:\n",
    "            return cur_node.pred\n",
    "        if cur_node.question.match(x_data) == 1:\n",
    "            return self.traverse(cur_node=cur_node.left, x_data=x_data)\n",
    "        else:\n",
    "            return self.traverse(cur_node=cur_node.right, x_data=x_data)\n",
    "    def print_acc(self, acc):\n",
    "        print(f'criterion = {self.criterion}')\n",
    "        print(f'max depth = {self.max_depth}')\n",
    "        print(f'acc       = {acc}')\n",
    "        print('====================')\n",
    "    \n",
    "    def predict(self, x_data):\n",
    "        miss_num = 0\n",
    "        total_num = len(x_data)\n",
    "        pred = []\n",
    "        for row in x_data:\n",
    "            ans = self.traverse(cur_node=self.root, x_data=row)\n",
    "            # print(f'cmp {ans} and {row[-1]}')\n",
    "            pred.append(ans)\n",
    "            if ans != row[-1]:\n",
    "                miss_num = miss_num + 1\n",
    "        accuracy = 1 - (miss_num / total_num)\n",
    "        self.print_acc(acc=accuracy)\n",
    "        return accuracy, pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_train = np.random.randint(50, size=(10,5))\n",
    "# my_train = np.hstack((my_train, np.random.randint(2, size=(10, 1))))\n",
    "# print(my_train)\n",
    "# clf_test3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "# clf_test3.fit(x_data=my_train[:, 0:5], y_data=my_train[:, -1])\n",
    "# clf_test3.print_tree(clf_test3.root, '')\n",
    "# clf_test3.get_feature_count()\n",
    "# x_test = np.array(my_train)\n",
    "# print(clf_test3.predict(x_test))\n",
    "# x = np.array([5,3,2,1])\n",
    "# x = x[np.newaxis, :]\n",
    "# print(x[:, -1])\n",
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.fit(x_data=x_train[:, 0:20], y_data=x_train[:, -1])\n",
    "# clf_depth3.print_tree(clf_depth3.root, '')\n",
    "clf_depth3.get_feature_count()\n",
    "acc, _ = clf_depth3.predict(x_test)\n",
    "\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "clf_depth10.fit(x_data=x_train[:, 0:20], y_data=x_train[:, -1])\n",
    "# clf_depth10.print_tree(clf_depth3.root, '')\n",
    "clf_depth10.get_feature_count()\n",
    "acc, _ = clf_depth10.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ram': 9, 'battery_power': 8, 'px_height': 10, 'talk_time': 4, 'sc_h': 2, 'wifi': 3, 'px_width': 5, 'm_dep': 1, 'mobile_wt': 1, 'touch_screen': 2, 'sc_w': 1, 'fc': 1}\n",
      "criterion = gini\n",
      "max depth = None\n",
      "acc       = 0.9433333333333334\n",
      "====================\n",
      "{'ram': 11, 'battery_power': 6, 'sc_w': 3, 'px_width': 4, 'three_g': 2, 'px_height': 6, 'talk_time': 2, 'sc_h': 1, 'mobile_wt': 1, 'n_cores': 1, 'm_dep': 1}\n",
      "criterion = entropy\n",
      "max depth = None\n",
      "acc       = 0.95\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=None)\n",
    "clf_gini.fit(x_data=x_train[:, 0:20], y_data=x_train[:, -1])\n",
    "# clf_depth3.print_tree(clf_depth3.root, '')\n",
    "clf_gini.get_feature_count()\n",
    "acc, _ = clf_gini.predict(x_test)\n",
    "\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=None)\n",
    "clf_entropy.fit(x_data=x_train[:, 0:20], y_data=x_train[:, -1])\n",
    "# clf_depth3.print_tree(clf_depth3.root, '')\n",
    "clf_entropy.get_feature_count()\n",
    "acc, _ = clf_entropy.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Question 3\n",
    "# Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_sorted = dict(sorted(clf_depth3.feature_count.items(), key=lambda item: item[1]))\n",
    "feature_names = feature_sorted.keys()\n",
    "print(feature_names)\n",
    "feature_counts = feature_sorted.values()\n",
    "print(feature_counts)\n",
    "\n",
    "x_pos = [x for x,_ in enumerate(feature_names)]\n",
    "plt.barh(x_pos, feature_counts, height=0.4)\n",
    "plt.ylabel('feature names')\n",
    "plt.xlabel('feature importance')\n",
    "plt.xticks(np.arange(max(feature_counts)+1))\n",
    "plt.yticks(x_pos, feature_names, rotation=45)\n",
    "plt.gca().grid(axis='x', which='major')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fi_gini_d3.png', dpi=300, transparent=False)\n",
    "plt.clf()\n",
    "\n",
    "feature_sorted = dict(sorted(clf_depth10.feature_count.items(), key=lambda item: item[1]))\n",
    "feature_names = feature_sorted.keys()\n",
    "print(feature_names)\n",
    "feature_counts = feature_sorted.values()\n",
    "print(feature_counts)\n",
    "\n",
    "x_pos = [x for x,_ in enumerate(feature_names)]\n",
    "plt.barh(x_pos, feature_counts, height=0.4)\n",
    "plt.ylabel('feature names')\n",
    "plt.xlabel('feature importance')\n",
    "plt.xticks(np.arange(max(feature_counts)+1))\n",
    "plt.yticks(x_pos, feature_names, rotation=45)\n",
    "plt.gca().grid(axis='x', which='major')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fi_gini_d10.png', dpi=300, transparent=False)\n",
    "plt.clf()\n",
    "\n",
    "feature_sorted = dict(sorted(clf_entropy.feature_count.items(), key=lambda item: item[1]))\n",
    "feature_names = feature_sorted.keys()\n",
    "print(feature_names)\n",
    "feature_counts = feature_sorted.values()\n",
    "print(feature_counts)\n",
    "\n",
    "x_pos = [x for x,_ in enumerate(feature_names)]\n",
    "plt.barh(x_pos, feature_counts, height=0.4)\n",
    "plt.ylabel('feature names')\n",
    "plt.xlabel('feature importance')\n",
    "plt.xticks(np.arange(max(feature_counts)+1))\n",
    "plt.yticks(x_pos, feature_names, rotation=45)\n",
    "plt.gca().grid(axis='x', which='major')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fi_entropy_d3.png', dpi=300, transparent=False)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4(working)\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators, max_features, bootstrap, criterion):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.use_bootstrap = bootstrap\n",
    "        self.criterion = criterion\n",
    "        if criterion == 'gini':\n",
    "            self.meas_func = gini\n",
    "        else:\n",
    "            self.meas_func = entropy\n",
    "        self.n_trees = None\n",
    "        return None\n",
    "\n",
    "    def draw_bootstrap(self, x_data):\n",
    "        # draw N data from x_data\n",
    "        n_rows = [x_data[np.random.randint(len(x_data))] for iter in range(len(x_data))]\n",
    "        return n_rows\n",
    "    \n",
    "    def fit(self, x_data, y_data):\n",
    "        self.max_features = np.floor(np.sqrt(len(x_data[0]-1)))\n",
    "        for b in range(self.n_estimators):\n",
    "            if self.use_bootstrap == True:\n",
    "                n_rows = self.draw_bootstrap(x_data=x_data)\n",
    "            else:\n",
    "                pass\n",
    "        pass\n",
    "    def print_acc(self, acc):\n",
    "        print(f'criterion = {self.criterion}')\n",
    "        print(f'max depth = {self.max_depth}')\n",
    "        print(f'acc       = {acc}')\n",
    "        print('====================')\n",
    "    def predict(self, x_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_10est = AdaBoost(n_estimators=10)\n",
    "ada_100est = AdaBoost(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, bootstrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = int(np.round(max_features))\n",
    "        self.use_bootstrap = bootstrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.n_trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.n_trees.append(DecisionTree(self.criterion, self.max_depth))\n",
    "        return None\n",
    "    \n",
    "    def fit(self, x_data, y_data):\n",
    "        for iter in range(self.n_estimators):\n",
    "            if self.use_bootstrap == True:\n",
    "                # choose $(max_features) features from data\n",
    "                n_cols = random.sample(range(x_data.shape[1]), k=self.max_features)\n",
    "                # draw N random samples from dataset\n",
    "                n_rows = np.random.randint(x_data.shape[0], size=len(x_data))\n",
    "                rows = x_data[n_rows]\n",
    "                rows = rows[:, n_cols]\n",
    "                # print(train_df.columns[n_cols])\n",
    "                # print(rows.shape)\n",
    "                self.n_trees[iter].fit(rows, y_data[n_rows])\n",
    "            else:\n",
    "                self.n_trees[iter].fit(x_data=x_data, y_data=y_data)\n",
    "        return\n",
    "    def print_acc(self, acc):\n",
    "        print(f'n estimators = {self.n_estimators}')\n",
    "        print(f'max features = {self.max_features}')\n",
    "        print(f'boostrap     = {self.use_bootstrap}')\n",
    "        print(f'criterion    = {self.criterion}')\n",
    "        print(f'max depth    = {self.max_depth}')\n",
    "        print(f'acc          = {acc}')\n",
    "        print('====================')\n",
    "    \n",
    "    def predict(self, x_data):\n",
    "        mis_count = 0\n",
    "        x_pred = []\n",
    "        for row in x_data:\n",
    "            row = row[np.newaxis, :]\n",
    "            vote_now = []\n",
    "            for tree_k in self.n_trees:\n",
    "                _, pred = tree_k.predict(x_data=row)\n",
    "                vote_now.append(pred)\n",
    "            label, cnt = np.unique(vote_now, return_counts=True)\n",
    "            vote_now = label[np.argmax(cnt)]\n",
    "            x_pred.append(vote_now)\n",
    "            if vote_now == row[:,-1]:\n",
    "                mis_count = mis_count + 1\n",
    "        acc = 1 - mis_count / len(x_data)\n",
    "        self.print_acc(acc)\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]), max_depth=None, criterion='gini')\n",
    "clf_10tree.fit(x_data=x_train[:, 0:20], y_data=x_train[:, -1])\n",
    "clf_10tree.predict(x_data=x_test)\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]), max_depth=None, criterion='gini')\n",
    "clf_100tree.fit(x_data=x_train[:, 0:20], y_data=x_train[:, -1])\n",
    "clf_100tree.predict(x_data=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = train_your_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a85b7bf7c779ed67bccf8ae1048f09ef25809d9114b21b047a7bffa30bad8e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

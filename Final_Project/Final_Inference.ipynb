{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import HuberRegressor, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code A has 386 samples to fill nan\n",
      "KNN imputing code A\n",
      "code B has 418 samples to fill nan\n",
      "KNN imputing code B\n",
      "code C has 391 samples to fill nan\n",
      "KNN imputing code C\n",
      "code D has 398 samples to fill nan\n",
      "KNN imputing code D\n",
      "code E has 429 samples to fill nan\n",
      "KNN imputing code E\n",
      "code F has 420 samples to fill nan\n",
      "KNN imputing code F\n",
      "code G has 373 samples to fill nan\n",
      "KNN imputing code G\n",
      "code H has 361 samples to fill nan\n",
      "KNN imputing code H\n",
      "code I has 377 samples to fill nan\n",
      "KNN imputing code I\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df_train, df_test):\n",
    "    data = pd.concat([df_train, df_test])\n",
    "    \n",
    "    data['m3_missing'] = data['measurement_3'].isnull().astype(np.int8)\n",
    "    data['m5_missing'] = data['measurement_5'].isnull().astype(np.int8)\n",
    "    data['area'] = data['attribute_2'] * data['attribute_3']\n",
    "    # data['loading'] = np.log(data['loading'])\n",
    "\n",
    "    # Select the variables used to generate null value of measurement_17 for each product code\n",
    "    # e.g. for product 'A', NaN values in measurement_17 is filled by a linear combination of (measurement_5, measurement_6, measurement_8)\n",
    "    full_fill_dict = {\n",
    "        'A': ['measurement_5','measurement_6','measurement_8'],\n",
    "        'B': ['measurement_4','measurement_5','measurement_7'],\n",
    "        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n",
    "        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n",
    "        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n",
    "        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n",
    "        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n",
    "        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n",
    "        'I': ['measurement_3','measurement_7','measurement_8']\n",
    "    }\n",
    "\n",
    "\n",
    "    features = [f for f in test.columns if 'measurement' in f] + ['loading']\n",
    "    for code in data['product_code'].unique(): # ('A', 'B', ..., 'I')\n",
    "        tmp = data[data['product_code'] == code]\n",
    "        column = full_fill_dict[code]\n",
    "        \n",
    "        # the data used to train the HuberRegressor, so all variables & `measurement_17` should exist <=> drop all NaN values\n",
    "        tmp_train = tmp[column + ['measurement_17']].dropna(how='any')\n",
    "        \n",
    "        # tmp_test = tmp[(all variables in `full_fill_dict[code]` are not NaN) & (the `measurement_17` is NaN in the row of data)]\n",
    "        tmp_test = tmp[(tmp[column].isnull().sum(axis=1) == 0) & (tmp['measurement_17'].isnull())]\n",
    "        print(f\"code {code} has {len(tmp_test)} samples to fill nan\")\n",
    "        \n",
    "        \n",
    "        model = HuberRegressor()\n",
    "        model.fit(tmp_train[column], tmp_train['measurement_17'])\n",
    "        # Fill the missing `measurement_17` values by the trained model\n",
    "        data.loc[(data['product_code'] == code) & (data[column].isnull().sum(axis=1) == 0) & (data['measurement_17'].isnull()), 'measurement_17'] = model.predict(tmp_test[column])\n",
    "        \n",
    "        # Other missing values are filled by KNNImputer\n",
    "        model2 = KNNImputer(n_neighbors=5)\n",
    "        print(f\"KNN imputing code {code}\")\n",
    "        data.loc[(data['product_code'] == code), features] = model2.fit_transform(data.loc[(data['product_code'] == code), features])\n",
    "    \n",
    "    data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n",
    "    \n",
    "    # [:df_train.shape[0], :] = rows[0:df_train.shape[0]] = [0, df_train.shape[0])\n",
    "    # [df_train.shape[0]:, :] = rows[df_train.shape[0]:len(rows)+1] = [df_train.shape[0], len(rows)]\n",
    "    df_train = data.iloc[:df_train.shape[0], :]\n",
    "    df_test = data.iloc[df_train.shape[0]:, :]\n",
    "    \n",
    "    # woe_encoder = WoEEncoder(variables=['attribute_0'])\n",
    "    # woe_encoder.fit(df_train, df_train['failure'])\n",
    "    # df_train = woe_encoder.transform(df_train)\n",
    "    # df_test = woe_encoder.transform(df_test)\n",
    "    encoder = LabelEncoder()\n",
    "    df_train['attribute_0'] = encoder.fit_transform(df_train['attribute_0'])\n",
    "    df_test['attribute_0'] = encoder.fit_transform(df_test['attribute_0'])\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = preprocessing(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scale(test_data, feats):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaled_test = scaler.fit_transform(test_data[feats])\n",
    "    \n",
    "    #back to dataframe\n",
    "    new_test = test_data.copy()\n",
    "    \n",
    "    new_test[feats] = scaled_test\n",
    "    \n",
    "    assert len(test_data) == len(new_test)\n",
    "    \n",
    "    return new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['failure'], axis=1)\n",
    "y = df_train.failure # y = df_train['failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure why `m5_missing` is not selected from the code above, but it has greater influence than other features\n",
    "select_features = ['loading', 'measurement_17', 'm5_missing', 'attribute_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################## 0 ##########################\n",
      "########################## 1 ##########################\n",
      "########################## 2 ##########################\n",
      "########################## 3 ##########################\n",
      "########################## 4 ##########################\n",
      "############# End of Classifier #############\n"
     ]
    }
   ],
   "source": [
    "# kf = GroupKFold(n_splits=5)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "lr_oof_1 = np.zeros(len(train))\n",
    "lr_oof_2 = np.zeros(len(train))\n",
    "lr_test = np.zeros(len(test))\n",
    "lr_auc = 0\n",
    "lr_acc = 0\n",
    "importance_list = []\n",
    "features = select_features\n",
    "\n",
    "filename = 'Model0.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "# for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y, groups=X['product_code'])):\n",
    "# Cross-Validation for 4 Logistic Regression Model\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f'########################## {fold_idx} ##########################')\n",
    "    # iloc : indexed_location (by int index ('int'))\n",
    "    # loc : location (by feature name ('object'))\n",
    "    x_test = df_test.copy()\n",
    "    \n",
    "    # Use StandardScaler to standardize the data\n",
    "    x_test = test_scale(x_test, features)\n",
    "        \n",
    "    lr_test += model.predict_proba(x_test[features])[:, -1] / 5\n",
    "\n",
    "print(f'############# End of Classifier #############')\n",
    "    \n",
    "submission['lr0'] = lr_test\n",
    "submission['rank0'] = rankdata(submission['lr0'])\n",
    "submission['failure'] = submission['rank0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['id', 'failure']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
